{
 "metadata": {
  "name": "",
  "signature": "sha256:c336c95d42ece244c6d2e3909c66b3bd9d67c49a9c913a91136a9aa726713da9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint as pp\n",
      "\n",
      "# Configure the necessary Spark environment                                                                                                             \n",
      "import os\n",
      "os.environ['SPARK_HOME'] = '/home/ubuntu/spark/'\n",
      "\n",
      "# And Python path                                                                                                                                        \n",
      "import sys\n",
      "#sys.path.insert(0, '/home/ubuntu/assertion/ia-py-assertion/src/main/python')\n",
      "\n",
      "\n",
      "from pyspark import SparkConf, SparkContext\n",
      "\n",
      "CLUSTER_URL = 'local'\n",
      "APP_NAME = 'assertion'\n",
      "\n",
      "\n",
      "conf = SparkConf().setMaster(CLUSTER_URL).setAppName(APP_NAME)          \n",
      "sc = SparkContext(conf=conf)                                       \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "records = sc.pickleFile('/data/xframe/data-files/proc-raw')\n",
      "print records.take(10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(36) FlatMappedRDD[255] at objectFile at NativeMethodAccessorImpl.java:-2 []\n",
        " |   /data/xframe/data-files/proc-raw HadoopRDD[254] at objectFile at NativeMethodAccessorImpl.java:-2 []\n",
        "[['M', '37', 'D', 'ND', '402013263003179', '5', '000010178000661621', '13263003179', '01', '', '3', '51', '2013266', '0', '', '', '485', '2', 'D', '990', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '485', '51', '131209', '12', '131002', '12', '130923', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', 'S77', 'S77', '0', '1', '', 'ef13afc32fbf16e599356bf15c262265becdd5c459c3bcac18c2fe57cc5577e0', 'ef13afc32fbf16e599356bf15c262265becdd5c459c3bcac18c2fe57cc5577e0', '', '5300289507', '', '', '', '53', '53', ' 010', '2', '1', '', '25', '1988', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000084148', '262.18', '0.00', '0.00', '262.18', '0.00', '0.00', '0.00', '0.00', '2013343', '', '20131210', '3080930', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131209', '20130923', '', '', '', '', '132590317', '999999', '7840', 'N', 'N', 'V053', 'N', 'N', '2449', 'N', 'N', '30000', 'N', 'N', '311', 'N', 'N', '', '', '', '20130905', '20130905', '20130905', '20130905', '20130905', '20130905', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'N', '', '', '', '', '', '', '', '', '   50', '', '2', 'C', '1', '20130905', '20130905', '1.0', '4', '05001', '8', ' 3', '000084148', 'N', '1', '90746', '', '', '', '0.00', 'V053', '0', '', '', '0', '0', '0.00', '', '3', '71.60', '0.00', '0.00', '0.00', '0.00', '11.89', '0.00', '0.00', '0.00', '0.00', '59.71', '   50'], ['M', '37', 'D', 'ND', '402013263003179', '5', '000010178000661621', '13263003179', '01', '', '3', '51', '2013266', '0', '', '', '485', '2', 'D', '990', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '485', '51', '131209', '12', '131002', '12', '130923', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', 'S77', 'S77', '0', '1', '', 'ef13afc32fbf16e599356bf15c262265becdd5c459c3bcac18c2fe57cc5577e0', 'ef13afc32fbf16e599356bf15c262265becdd5c459c3bcac18c2fe57cc5577e0', '', '5300289507', '', '', '', '53', '53', ' 010', '2', '1', '', '25', '1988', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000084148', '262.18', '0.00', '0.00', '262.18', '0.00', '0.00', '0.00', '0.00', '2013343', '', '20131210', '3080930', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131209', '20130923', '', '', '', '', '132590317', '999999', '7840', 'N', 'N', 'V053', 'N', 'N', '2449', 'N', 'N', '30000', 'N', 'N', '311', 'N', 'N', '', '', '', '20130905', '20130905', '20130905', '20130905', '20130905', '20130905', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'N', '', '', '', '', '', '', '', '', '   50', '', '3', 'C', '1', '20130905', '20130905', '1.0', '4', '05001', '8', ' 3', '000084148', 'N', '1', '90471', '', '', '', '0.00', 'V053', '0', '', '', '0', '0', '0.00', '', '3', '48.00', '0.00', '0.00', '0.00', '0.00', '33.54', '0.00', '0.00', '0.00', '0.00', '14.46', '   50'], ['M', '36', 'M', 'ND', '402013340002934', '8', '000010178000661622', '13340002934', '01', '', '1', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131209', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '141.58', '0.00', '0.00', '141.58', '0.00', '100.98', '0.00', '0.00', '2013343', '', '20131210', '3080930', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131209', '0', '', '', '', '05', '133300331', '999999', '4659', 'N', 'N', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '20131119', '20131119', '20131119', '20131119', '20131119', '20131119', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '1', 'C', '1', '20131119', '20131119', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '99213', '', '', '', '0.00', '4659', '0', '', '', '0', '0', '0.00', '', '3', '141.58', '100.98', '0.00', '0.00', '0.00', '41.60', '100.98', '0.00', '0.00', '0.00', '100.98', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '1', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '99392', '25', '', '', '0.00', 'V202', '0', '', '', '0', '0', '0.00', '', '3', '208.00', '145.97', '0.00', '0.00', '0.00', '62.03', '145.97', '0.00', '0.00', '0.00', '145.97', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '2', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90700', 'SL', '', '', '0.00', 'V061', '0', '', '', '0', '0', '0.00', '', '5', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '3', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90471', '', '', '', '0.00', 'V061', '0', '', '', '0', '0', '0.00', '', '5', '13.90', '13.90', '0.00', '0.00', '0.00', '0.00', '13.90', '0.00', '0.00', '0.00', '14.46', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '4', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90648', 'SL', '', '', '0.00', 'V0381', '0', '', '', '0', '0', '0.00', '', '5', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '5', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90472', '', '', '', '0.00', 'V0381', '0', '', '', '0', '0', '0.00', '', '5', '13.90', '13.90', '0.00', '0.00', '0.00', '0.00', '13.90', '0.00', '0.00', '0.00', '14.46', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '6', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90669', 'SL', '', '', '0.00', 'V0382', '0', '', '', '0', '0', '0.00', '', '5', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '95.48', '  N14'], ['M', '36', 'M', 'ND', '402013353002706', '0', '000010178000661622', '13353002706', '01', '', '9', '50', '0', '0', '', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '', '0', '', '50', '131223', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '', '0', '123', '123', '0', '0', '', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '917447070ed2c728a64f927699ab5abf8ef34555ed810e04cf8302a3e7fccb6b', '', '5300289507', '', '', '', '53', '53', ' 003', '2', '1', '', '1', '2012', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', '588', ' 3', '2', '', '', 'N', '', '0', '', 'N', 'N', 'N', '', '', '', '', '1', '0', '', '000010178', 'CR', '70', '50', '1', '588013', ' 0', 'N', '000010178', '', '000016461', '263.60', '0.00', '0.00', '263.60', '0.00', '187.67', '0.00', '0.00', '2013357', '', '20131224', '3082693', '0', '0', '0.00', '', '0', '', '', '', 'E', '20131223', '0', '', '', '', '05', '133460372', '999999', 'V202', 'N', 'N', 'V0381', 'N', 'N', 'V0382', 'N', 'N', 'V0481', 'N', 'N', 'V061', 'N', 'N', '', '', '', '20131202', '20131202', '20131202', '20131202', '20131202', '20131202', 'Y', 'Y', '0', '0', '0', '', '0', '0', '0', '0', '0', '0', 'Y', '', '', '', '', '', '', '', '', '', '', '7', 'C', '1', '20131202', '20131202', '1.0', '4', '05301', '0', ' 3', '000016461', 'N', '1', '90472', '', '', '', '0.00', 'V0382', '0', '', '', '0', '0', '0.00', '', '5', '13.90', '13.90', '0.00', '0.00', '0.00', '0.00', '13.90', '0.00', '0.00', '0.00', '14.46', '  N14']]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "col = records.map(lambda row: row[97],preservesPartitioning=True)\n",
      "print 'col'\n",
      "print col.take(10)\n",
      "b = col.map(lambda v: v == '2012', preservesPartitioning=True)\n",
      "print 'b'\n",
      "print b.take(10)\n",
      "print b.toDebugString()\n",
      "print 'records'\n",
      "print records.toDebugString()\n",
      "#z = records.zip(b)\n",
      "z = b.zip(records)\n",
      "print z.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "col\n",
        "['1988', '1988', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012']\n",
        "b\n",
        "[False, False, True, True, True, True, True, True, True, True]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(36) PythonRDD[338] at RDD at PythonRDD.scala:43 []\n",
        " |   FlatMappedRDD[255] at objectFile at NativeMethodAccessorImpl.java:-2 []\n",
        " |   /data/xframe/data-files/proc-raw HadoopRDD[254] at objectFile at NativeMethodAccessorImpl.java:-2 []\n",
        "records\n",
        "(36) FlatMappedRDD[255] at objectFile at NativeMethodAccessorImpl.java:-2 []\n",
        " |   /data/xframe/data-files/proc-raw HadoopRDD[254] at objectFile at NativeMethodAccessorImpl.java:-2 []\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 160.0 failed 1 times, most recent failure: Lost task 0.0 in stage 160.0 (TID 364, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (1, 100)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-36-505c7bad9353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#z = records.zip(b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 160.0 failed 1 times, most recent failure: Lost task 0.0 in stage 160.0 (TID 364, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (1, 100)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Try it with manufacturered data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recs = sc.parallelize(range(0, 1000))\n",
      "print 'recs'\n",
      "print recs.toDebugString()\n",
      "#print recs.take(10)\n",
      "\n",
      "evens = recs.map(lambda n: n % 2 == 0)\n",
      "print 'evens'\n",
      "print evens.toDebugString()\n",
      "#evens.take(10)\n",
      "z = recs.zip(evens)\n",
      "print 'z'\n",
      "print z.take(10)\n",
      "filtered = z.filter(lambda row: row[1]).keys()\n",
      "print 'filtered'\n",
      "print filtered.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "recs\n",
        "(1) ParallelCollectionRDD[685] at parallelize at PythonRDD.scala:364 []\n",
        "evens\n",
        "(1) PythonRDD[686] at RDD at PythonRDD.scala:43 []\n",
        " |  ParallelCollectionRDD[685] at parallelize at PythonRDD.scala:364 []\n",
        "z\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 257.0 failed 1 times, most recent failure: Lost task 0.0 in stage 257.0 (TID 461, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (100, 1)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-97-c4473061b255>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'z'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'filtered'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 257.0 failed 1 times, most recent failure: Lost task 0.0 in stage 257.0 (TID 461, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (100, 1)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recs = sc.parallelize(range(0, 10))\n",
      "\n",
      "even = recs.map(lambda n: n % 2 == 0)\n",
      "#print even.toDebugString()\n",
      "#even.take(10)\n",
      "\n",
      "z = recs.zip(even)\n",
      "print z.take(10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, True), (1, False), (2, True), (3, False), (4, True), (5, False), (6, True), (7, False), (8, True), (9, False)]\n"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the sample in the documentation\n",
      "x = sc.parallelize(range(0,5))\n",
      "y = sc.parallelize(range(1000, 1005))\n",
      "x.zip(y).collect()\n",
      "#[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 119,
       "text": [
        "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is a variation of the sample in the documentation\n",
      "x = sc.parallelize(range(0,5))\n",
      "y = x.map(lambda x: x+1000, preservesPartitioning=True)\n",
      "# This fails if either of the next two are uncommented\n",
      "#y.take(10)\n",
      "y.toDebugString()\n",
      "x.zip(y).collect()\n",
      "#[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o5453.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 301.0 failed 1 times, most recent failure: Lost task 0.0 in stage 301.0 (TID 505, localhost): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:713)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.foreach(RDD.scala:709)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.to(RDD.scala:709)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.toBuffer(RDD.scala:709)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.toArray(RDD.scala:709)\n\tat org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)\n\tat org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-132-59740f1d5654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#y.take(10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m         \"\"\"\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5453.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 301.0 failed 1 times, most recent failure: Lost task 0.0 in stage 301.0 (TID 505, localhost): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:713)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.foreach(RDD.scala:709)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.to(RDD.scala:709)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.toBuffer(RDD.scala:709)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.toArray(RDD.scala:709)\n\tat org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)\n\tat org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(0, 1000))\n",
      "y = x.map(lambda x: x+1000, preservesPartitioning=True)\n",
      "print y.toDebugString()\n",
      "z = x.zip(y)\n",
      "print z.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1) PythonRDD[911] at RDD at PythonRDD.scala:43 []\n",
        " |  ParallelCollectionRDD[910] at parallelize at PythonRDD.scala:364 []\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 314.0 failed 1 times, most recent failure: Lost task 0.0 in stage 314.0 (TID 518, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (100, 1)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-146-6e2147e450a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 314.0 failed 1 times, most recent failure: Lost task 0.0 in stage 314.0 (TID 518, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/home/ubuntu/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 227, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark/python/pyspark/rdd.py\", line 1107, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 297, in load_stream\n    \" in pair: (%d, %d)\" % (len(keys), len(vals)))\nValueError: Can not deserialize RDD with different number of items in pair: (100, 1)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}